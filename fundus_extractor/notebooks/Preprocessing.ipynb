{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "from typing import Tuple\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "import skimage\n",
    "\n",
    "import torch\n",
    "import torchvision\n",
    "import torchvision.transforms as T\n",
    "\n",
    "sys.path.append('../..')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "%autoreload 2\n",
    "from fundus_extractor.utils.general import imshow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ImageOutOfDistributionError(Exception):\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "def resize_image(image: torch.Tensor, image_resize_size: int) -> torch.Tensor:\n",
    "    return T.functional.resize(image, (image_resize_size, image_resize_size))\n",
    "\n",
    "\n",
    "def cut_image(image: torch.Tensor, threshold: float = 0.2, min_gray: float = 0.05)-> torch.Tensor:\n",
    "    gray_image = T.functional.rgb_to_grayscale(image) / 255\n",
    "    threshold *= torch.mean(gray_image).item()\n",
    "\n",
    "    # Threshold the grayscale image\n",
    "    binary_mask = (gray_image > threshold).float()[0]\n",
    "    if binary_mask.shape[0] < 150 or binary_mask.shape[1] < 150:\n",
    "        raise ImageOutOfDistributionError()\n",
    "\n",
    "    cols = torch.mean(binary_mask, dim=0) > min_gray\n",
    "    left, right = torch.min(torch.where(cols)[0]), torch.max(torch.where(cols)[0])\n",
    "\n",
    "    rows = torch.mean(binary_mask, dim=1) > min_gray\n",
    "    top, bottom = torch.min(torch.where(rows)[0]), torch.max(torch.where(rows)[0])\n",
    "\n",
    "    # Crop the image based on the contour boundaries\n",
    "    return image[:, top:bottom, left:right]\n",
    "\n",
    "\n",
    "def pad_to_square(image: torch.Tensor) -> torch.Tensor:\n",
    "    _, width, height = image.shape\n",
    "\n",
    "    # Determine the maximum dimension\n",
    "    max_dim = max(width, height)\n",
    "\n",
    "    # Calculate the padding amounts\n",
    "    pad_width = max_dim - width\n",
    "    pad_height = max_dim - height\n",
    "\n",
    "    # Calculate the padding values for top, bottom, left, and right\n",
    "    pad_top = pad_height // 2\n",
    "    pad_bottom = pad_height - pad_top\n",
    "    pad_left = pad_width // 2\n",
    "    pad_right = pad_width - pad_left\n",
    "\n",
    "    # Pad the image with zeros\n",
    "    return torch.nn.functional.pad(image, (pad_top, pad_bottom, pad_left, pad_right))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels_path = '/home/ludwig-graef/Workplace/Master_Thesis/Datasets/archive/trainLabels.csv'\n",
    "labels_df = pd.read_csv(labels_path)\n",
    "labels_df[['id', 'left_or_right']] = labels_df['image'].str.split('_', expand=True)\n",
    "\n",
    "# Drop the original \"image\" column\n",
    "labels_df.drop('image', axis=1, inplace=True)\n",
    "labels_df.head(10)\n",
    "\n",
    "labels_df[(labels_df['id'] == '10') & (labels_df['left_or_right'] == 'left')]['level'].item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2000/2000 [03:28<00:00,  9.59it/s]\n"
     ]
    }
   ],
   "source": [
    "image_dir = '/home/ludwig-graef/Workplace/Master_Thesis/Datasets/archive/train/split_001'\n",
    "save_dir = '/home/ludwig-graef/Workplace/Master_Thesis/Datasets/Fundus_01/data'\n",
    "\n",
    "for image_file_name in tqdm(os.listdir(image_dir)[:2000]):\n",
    "    id, left_or_right = image_file_name.split('_')\n",
    "    left_or_right = left_or_right.split('.')[0]\n",
    "    class_label = labels_df[(labels_df['id'] == id) & (labels_df['left_or_right'] == left_or_right)]['level'].item()\n",
    "\n",
    "    save_dir_image = os.path.join(save_dir, left_or_right, f'class_{class_label}')\n",
    "    save_path_image = os.path.join(save_dir_image, f'{id}.jpeg')\n",
    "    image_path = os.path.join(image_dir, image_file_name)\n",
    "    os.makedirs(save_dir_image, exist_ok=True)\n",
    "\n",
    "    try:\n",
    "        image_raw = torchvision.io.read_image(image_path)\n",
    "        cropped_image = cut_image(image_raw)\n",
    "        padded_image = pad_to_square(cropped_image)\n",
    "        resized_image = resize_image(padded_image, 224)\n",
    "        torchvision.utils.save_image(resized_image / 255, save_path_image)\n",
    "    except ImageOutOfDistributionError as e:\n",
    "        imshow(torchvision.io.read_image(image_path))\n",
    "        print(image_file_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "m_t",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.17"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
